# ğŸ§  Text-to-Head-Motion Generation from Text

## ğŸ“– Overview
This project performs **text-to-motion generation**, converting textual descriptions into **3D head motion sequences** and rendering them as **video animations**.  

- âš™ï¸ **Pretrained text encoder (CLIP)** â€” converts text prompts into embeddings  
- ğŸ¯ **Motion generation network** â€” predicts per-frame motion parameters  
- ğŸ¥ **3D rendering using FLAME template** (fallback to 2D dot visualization if unavailable)  
- ğŸ“‰ **MSE Loss** and **Adam optimizer** for training motion generation  
- ğŸ§± **Modular architecture** â€” clear separation of model, training, inference, and rendering  
- ğŸ“Š **Real-time loss logging** during training  
- ğŸŒ Optional **video output** for visualization  

---

## ğŸ§© Libraries
- **PyTorch** â€“ model, training, and inference  
- **Transformers** â€“ CLIP text encoder  
- **numpy**, **scipy** â€“ data handling  
- **torch.utils.data** â€“ dataset and dataloader  
- **Pillow**, **imageio**, **opencv-python** â€“ video and image handling  
- **trimesh**, **pyrender**, **pyglet** â€“ optional 3D rendering  
- **face-alignment** â€“ for FLAME/DECA landmarks  
- **tqdm** â€“ progress bars  

---

## âš™ï¸ Requirements

- Python **3.15+**
- Recommended editor: **VS Code**

---

## ğŸ“¦ Installation

- Clone the repository
```bash
git clone https://github.com/hurkanugur/Text2Video.git
```

- Navigate to the `Text2Video` directory
```bash
cd Text2Video
```

- Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ”§ Setup Python Environment in VS Code

1. `View â†’ Command Palette â†’ Python: Create Environment`  
2. Choose **Venv** and your **Python version**  
3. Select **requirements.txt** to install dependencies  
4. Click **OK**

---

## ğŸ“‚ Project Structure

```bash
assets/
â””â”€â”€ flame_head_template.png           # Flame head model

data/
â””â”€â”€ celebv-text                       # Training datasets

output/
â”œâ”€â”€ mesh                              # Generated mesh file
â””â”€â”€ video                             # Generated video file

model/
â””â”€â”€ text_2_motion_model.pt            # Trained model

src/
â”œâ”€â”€ config.py                         # Paths, hyperparameters, ...
â”œâ”€â”€ dataset.py                        # Data loading & preprocessing
â”œâ”€â”€ train.py                          # Training pipeline
â”œâ”€â”€ inference.py                      # Inference pipeline
â”œâ”€â”€ model.py                          # Motion generation model (text â†’ motion parameters)
â””â”€â”€ text_encoder.py                   # Text encoder model (text â†’ embeddings)

main/
â”œâ”€â”€ main_train.py                     # Entry point for training
â””â”€â”€ main_inference.py                 # Entry point for inference

requirements.txt                      # Python dependencies
```

---

## ğŸ“‚ Model Architecture

```bash
Input Text
    â†“
Pretrained CLIP Text Encoder
    â†“
Text Embedding (feature vector)
    â†“
Text2MotionModel (MLP)
    â†“
Per-frame Motion Parameters (FLAME)
    â†“
3D Rendering (pyrender) or 2D fallback
    â†“
Generated Video Output
```

---

## ğŸ“‚ Train the Model
Navigate to the project directory:
```bash
cd Text2Video
```

Run the training script:
```bash
python -m main.main_train
```
or
```bash
python3 -m main.main_train
```

---

## ğŸ“‚ Run Inference / Make Predictions
Navigate to the project directory:
```bash
cd Text2Video
```

Run the app:
```bash
python -m main.main_inference
```
or
```bash
python3 -m main.main_inference
```
